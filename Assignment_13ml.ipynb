{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMMFFw8MkSVD7d836RoxF2O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# ASSIGNMENT 13\n","OLUYEMI OLUWOYE\n","\n","https://github.com/OLUYEMI84/cola-GITHUB-demo-OO.git"],"metadata":{"id":"CY0v8-wp9I_B"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"NFXSxefJpDhe","executionInfo":{"status":"ok","timestamp":1751151089008,"user_tz":360,"elapsed":514,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}}},"outputs":[],"source":["import requests\n","\n","# Use a reliable URL from Gutenberg\n","url = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\"\n","response = requests.get(url)\n","text = response.text\n","\n","\n"]},{"cell_type":"code","source":["# Confirm download\n","print(f\"Downloaded text length: {len(text)}\")\n","print(\"First 500 characters:\")\n","print(text[:500])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4eOfAz2vDMp","executionInfo":{"status":"ok","timestamp":1751151110216,"user_tz":360,"elapsed":10,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}},"outputId":"e53881ae-f7ec-4194-a9a7-cca226e0b61c"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloaded text length: 167674\n","First 500 characters:\n","﻿The Project Gutenberg eBook of Alice's Adventures in Wonderland\r\n","    \r\n","This ebook is for the use of anyone anywhere in the United States and\r\n","most other parts of the world at no cost and with almost no restrictions\r\n","whatsoever. You may copy it, give it away or re-use it under the terms\r\n","of the Project Gutenberg License included with this ebook or online\r\n","at www.gutenberg.org. If you are not located in the United States,\r\n","you will have to check the laws of the country where you are located\r\n","befo\n"]}]},{"cell_type":"code","source":["# Remove header/footer\n","\n","start = text.find(\"CHAPTER I\")\n","if start != -1:\n","    text = text[start:]\n","else:\n","    print(\"Start marker not found. Using full text.\")\n"],"metadata":{"id":"5zKtH1OYp8ca","executionInfo":{"status":"ok","timestamp":1751151426215,"user_tz":360,"elapsed":20,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# Show a preview\n","print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPM_acS9wmLp","executionInfo":{"status":"ok","timestamp":1751151446408,"user_tz":360,"elapsed":27,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}},"outputId":"e92fdbe6-4414-44ea-8f3c-48ddc5b3d8b5"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["CHAPTER I.     Down the Rabbit-Hole\r\n"," CHAPTER II.    The Pool of Tears\r\n"," CHAPTER III.   A Caucus-Race and a Long Tale\r\n"," CHAPTER IV.    The Rabbit Sends in a Little Bill\r\n"," CHAPTER V.     Advice from a Caterpillar\r\n"," CHAPTER VI.    Pig and Pepper\r\n"," CHAPTER VII.   A Mad Tea-Party\r\n"," CHAPTER VIII.  The Queen’s Croquet-Ground\r\n"," CHAPTER IX.    The Mock Turtle’s Story\r\n"," CHAPTER X.     The Lobster Quadrille\r\n"," CHAPTER XI.    Who Stole the Tarts?\r\n"," CHAPTER XII.   Alice’s Evidence\r\n","\r\n","\r\n","\r\n","\r\n","CHAPTER I.\r\n","Down the Rabbit-Hole\r\n","\r\n","\r\n","Alice was beginning to get very tired of sitting by her sister on the\r\n","bank, and of having nothing to do: once or twice she had peeped into\r\n","the book her sister was reading, but it had no pictures or\r\n","conversations in it, “and what is the use of a book,” thought Alice\r\n","“without pictures or conversations?”\r\n","\r\n","So she was considering in her own mind (as well as she could, for the\r\n","hot day made her feel very sleepy and stupid), whether the pleasure of\r\n","making a daisy-chain would \n"]}]},{"cell_type":"code","source":["#  Tokenization\n","import tensorflow as tf\n","import numpy as np\n","\n","# Char-level\n","vocab = sorted(set(text))\n","char2idx = {u:i for i, u in enumerate(vocab)}\n","idx2char = np.array(vocab)\n","\n","text_as_int = np.array([char2idx[c] for c in text])\n","\n","print(f\"Vocabulary size: {len(vocab)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tVEqBAlgwx6c","executionInfo":{"status":"ok","timestamp":1751151515304,"user_tz":360,"elapsed":4601,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}},"outputId":"658bf087-129d-4f9a-f7f2-aaf3f5bf2730"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocabulary size: 90\n"]}]},{"cell_type":"code","source":["# building dataset\n","seq_length = 100\n","examples_per_epoch = len(text_as_int)//(seq_length + 1)\n","\n","char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n","\n","sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)\n","\n","def split_input_target(chunk):\n","    return chunk[:-1], chunk[1:]\n","\n","dataset = sequences.map(split_input_target)\n","\n","# Batch + buffer\n","BATCH_SIZE = 64\n","BUFFER_SIZE = 10000\n","\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n"],"metadata":{"id":"9oedqiInyB9A","executionInfo":{"status":"ok","timestamp":1751151913266,"user_tz":360,"elapsed":81,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["dataset"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XqkcAhBZyeoY","executionInfo":{"status":"ok","timestamp":1751151959026,"user_tz":360,"elapsed":19,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}},"outputId":"5f4a5080-cbf6-40cc-cecd-876c417d63b1"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<_BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# building model\n","vocab_size = len(vocab)\n","embedding_dim = 256\n","rnn_units = 1024\n","\n","def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n","    return tf.keras.Sequential([\n","        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=None),\n","        tf.keras.layers.GRU(rnn_units,\n","                            return_sequences=True,\n","                            stateful=False,  # set to False unless you handle states manually\n","                            recurrent_initializer='glorot_uniform'),\n","        tf.keras.layers.Dense(vocab_size)\n","    ])\n","\n","model = build_model(vocab_size, embedding_dim, rnn_units, BATCH_SIZE)\n","\n"],"metadata":{"id":"qfhN-wyDyrhY","executionInfo":{"status":"ok","timestamp":1751152063280,"user_tz":360,"elapsed":54,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["# training\n","\n","def loss(labels, logits):\n","    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","\n","model.compile(optimizer='adam', loss=loss)\n","\n","history = model.fit(dataset, epochs=10)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tmaa203bzEos","executionInfo":{"status":"ok","timestamp":1751153618624,"user_tz":360,"elapsed":1522282,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}},"outputId":"af1948be-ad3f-46b1-ab07-a5341770e2f2"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 6s/step - loss: 4.3405\n","Epoch 2/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 6s/step - loss: 2.7610\n","Epoch 3/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - loss: 2.4073\n","Epoch 4/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 6s/step - loss: 2.2378\n","Epoch 5/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - loss: 2.1078\n","Epoch 6/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 6s/step - loss: 1.9807\n","Epoch 7/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 6s/step - loss: 1.8654\n","Epoch 8/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 6s/step - loss: 1.7684\n","Epoch 9/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m150s\u001b[0m 6s/step - loss: 1.6772\n","Epoch 10/10\n","\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 6s/step - loss: 1.5890\n"]}]},{"cell_type":"code","source":["def generate_text(model, start_string):\n","    input_eval = tf.expand_dims([char2idx[c] for c in start_string], 0)\n","    text_generated = []\n","    temperature = 1.0\n","\n","    for _ in range(500):\n","        predictions = tf.squeeze(model(input_eval), 0) / temperature\n","        predicted_id = tf.random.categorical(predictions, 1)[-1, 0].numpy()\n","        input_eval = tf.expand_dims([predicted_id], 0)\n","        text_generated.append(idx2char[predicted_id])\n","\n","    return start_string + ''.join(text_generated)\n","\n","print(generate_text(model, \"Alice \"))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNQ4B7vc6RGT","executionInfo":{"status":"ok","timestamp":1751154047142,"user_tz":360,"elapsed":10652,"user":{"displayName":"Yemi Oluwoye","userId":"15750742042367202196"}},"outputId":"a903cadd-1f8c-48f0-af6f-2c2a24ac53d7"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Alice we,”, atho.\r\n","Q; t bU D•* te was\r\n","ttr arzhatwhegese s ak omm, cqE)[A!™\r\n","I “FEcaribeles WAblaton s ifus corcond Mathese w at: qY!—R™\r\n","anifesig?-ynise,\r\n","c™ Awhomag, seur f im ke dupo?1: s w “CO” ar,” amabow w\r\n","itld TFo(DO/_L2ny Y)D’viga•Ficobthe,” pe,”\r\n","PTl, inorus putexin\r\n","y\r\n","EG'6ùxh, as onogheed w\r\n","ppd trnbelive.FYoty pish ngowwabo; h the, whetrminop Om eg_\r\n","natechy rer oxenedoke h w\r\n","domenitoculen L y, moprpe d sthere’\n","ced me imond tinghokn MI ar, _‘Xù _Sth, dalar, qE rgraro—(A9);,”\r\n","songing s M\n"]}]},{"cell_type":"markdown","source":["\n","**Introduction to Generative AI and Its Significance**\n","\n","Generative AI refers to artificial intelligence systems designed to produce new content such as text, images, music, or code. Unlike traditional models focused on classification or regression, generative models learn the underlying data distribution and can create realistic outputs that resemble human-produced data.\n","\n","Generative AI plays a key role in applications like chatbots, creative writing, code assistants, image generation, and even drug discovery. Its capacity to assist, augment, or automate creative tasks marks it as a transformative technology.\n","\n","***Description of GPT Architecture and Functionality***\n","\n","Generative Pre-trained Transformers (GPTs) are large language models built on transformer decoder architectures. The key components of GPTs are:\n","\n","Stacked transformer blocks with masked self-attention layers, which ensure the model only uses previous tokens when predicting the next token.\n","\n","Multi-head self-attention, allowing the model to learn different types of relationships between tokens simultaneously.\n","\n","Position embeddings, since transformers do not have built-in sequence order awareness.\n","\n","Feedforward neural networks at each transformer layer, adding non-linear transformations.\n","\n","GPTs generate text by converting input text into tokens, predicting the next token's probability distribution, and sampling from this distribution repeatedly until a stopping criterion is met.\n","\n","***Methodology and Findings***\n","\n","I built a simple character-level text generation model using TensorFlow. The dataset was Alice’s Adventures in Wonderland from Project Gutenberg. Below is the process:\n","\n","Preprocessing: I converted characters into integer indices and sliced the text into overlapping sequences (input = first 100 characters; target = next 100 characters shifted by one).\n","\n","Model: A sequential model with an embedding layer, a GRU layer (1024 units), and a dense output layer predicting the next character.\n","\n","Training: I trained the model for 10 epochs. The model learned to mimic the writing style of the dataset, generating coherent, though often nonsensical, text.\n","\n","\n","***Applications of Generative AI and Demonstration***\n","\n","Generative AI powers many real-world applications:\n","\n","Content creation: Drafting articles, poetry, dialogue (e.g., ChatGPT).\n","\n","Code generation: Assisting programmers.\n","\n","Art and design: Generating images or music.\n","\n","Education: Creating practice questions or explanations.\n","\n","Our hands-on model demonstrates how even a simple generative model can produce creative text snippets that could inspire writers or be used for games.\n","\n","***Ethical Considerations and Potential Solutions***\n","\n","Generative AI raises several ethical concerns:\n","\n","Bias propagation: AI can amplify biases in its training data.\n","\n","Misinformation: Generated content might be mistaken for verified facts.\n","\n","Plagiarism concerns: AI may replicate training data too closely.\n","\n","Potential solutions:\n","\n","Careful curation and filtering of training data.\n","\n","Implementing content filters or moderation layers.\n","\n","Marking AI-generated content (watermarking or metadata).\n","\n","***Conclusion***\n","\n","Generative AI, exemplified by models like GPT, represents a major advance in machine learning’s creative potential. The simple text generation project highlights the core mechanics behind this technology and its capacity for creative output."],"metadata":{"id":"XZ7BDPpL3Y-l"}}]}